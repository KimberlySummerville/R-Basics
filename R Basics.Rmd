---
output:
  word_document: default
  html_document: default
---

# Part 1

## Is there a significant relationship between the amount of time spent reading and the time spent watching television?

A. 
Covariance as a statistical tool is used in order to find the relationship between variables, whether positively or negatively correlated. When the covariance is positive, say with the variables TimeTV and Happiness, this means that there is a positive relationship between the two – as the time spent watching TV goes up, so does the reported happiness of the individual. When the covariance is negative, like with TimeReading and Happiness, the means that as the time spent reading goes up, the reported happiness goes down. While it does not indicate the strength of the relationship, it can help in predicting the direction of estimated values.

Covariance of data in Student Survey
```{r}
library(dplyr)
library(ggm)
stud_surv <- data.frame(read.csv("Student Survey.csv"))
cov(stud_surv)
```


B. 
TimeReading is measured in hours, TimeTV is measured in minutes. While TimeReading and TimeTV are both measurements of time, there is a conversion that needs to be done in order to have them measured the same way. In respect to the relationships represented by covariance, this does not have an effect – the negative relationships stay negative, and positive relationships stay positive. What is will effect, however, is the magnitude of the covariance, which, as covariance is unscaled, can be difficult to interpret. For uniformity of data, as well as reducing the effects on future calculations, the best practice is to convert all similar variables (i.e. time, distance, temperature, etc) into the same measurements (i.e. hrs, km, Kelvins, etc). In this case, converting the TimeTV column to hours would be a better alternative to the current measurements.
Happiness appears to be a percentage measure on a scale of 0 - 100%, and Gender is currently an integer. While Gender can not be quantified by numbers, factored variables cannot be included in a covariance calculation.

New data frame with converted values of TimeTV
```{r}
stud_surv_hours <- stud_surv %>% mutate(TimeTV = TimeTV/60)
stud_surv_hours
```

Covariance of converted data 
``` {r}
cov(stud_surv_hours)
```

C.
The correlation test chosen is the Pearson Correlation, as not only is its range specified between -1 and 1, but it can indicate the strength of relationships as well as direction. As the covariance calculation calculates the direction of correlation, positive or negative, I would expect the Pearson test to yield the same results as the calculation performed in section B.

D.1 
Pearson Correlation of all variables

```{r}
cor(stud_surv_hours, use = "everything", method = "pearson")
```

D.2 
Pearson Correlation of TimeTV vs. TimeReading

```{r}
cor.test(stud_surv_hours$TimeTV, stud_surv_hours$TimeReading, use = "everything", method = "pearson")
```

D.3 
Pearson Correlation of TimeTV vs. TimeReading with Confidence Interval at 99%

```{r}
cor.test(stud_surv_hours$TimeTV, stud_surv_hours$TimeReading, use = "everything", method = "pearson", conf.level = .99)
```

D.4 
The negative Pearson correlation coefficient points to a negative relationship between time spent watching TV and time spent reading - as one increases, the other decreases. The magnitude of the correlation is demonstrated by how close to -1 or 1 it is, with .5/-.5 being a benchmark for significance. With a coefficient of -0.8830677, this indicates a very significant relationship. 
Additionally, both the confidence intervals at 95% and 99% never cross 0, indicating a high confidence that the two are negatively correlated.

E. 
The correlation coefficient is -0.8830677 as seen in D.3. As time spent reading itself can be influenced by a number of outside factors other than time spent watching TV, we can take the correlation coefficient one step further by squaring it and finding the coefficient of determination (R^2), which can give a measure of variability in one variable that is shared by the other. R^2 in this case is .779809, which, converted to a percentage, is 77.98%, meaning that time spent watching TV shares 77.98% of variability with time spent reading. The transalates into time spent watching TV accounting for 77.98% of the variation in time spent reading, and only 22.02% of the variation in time spent reading being from outside factors. 

F.
With the high coefficient of determination and strong relationship determined by the correlation coefficient, I am comfortable saying that the time spent reading was negatively impacted by time spent watching TV. While there are always be outside forces acting on the individuals, one explanation is the fact that there is a limited number of hours per day, and when there is more time spent watching TV, there is less time available for reading. Even with outside factors, though, the data shows that when time spent watching TV is increased, time spent reading readily decreases.

G.
The partial correlation between time spent watching TV and Happiness is .6435158, which is not only positive, but is also about .5, pointing towards a strong relationship. This indicates that the more time spent watching TV, the happier an individual is, regardless of Gender. This could also be a determining factor in the negative relationship between time spent watching TV and time spent reading - if time spent watching TV increases happiness, a person will gravitate towards that. Further analysis between time spent reading and happiness could also support this idea. 

Partial Correlation between TimeTV and Happiness, controlling for Gender
```{r}
pcor(c("TimeTV", "Happiness", "Gender"), var(stud_surv_hours))
```

```{r}
library(ggplot2)
library(ggm)
```

# Part 2

A. According to the below plot, we can see that as the number of siblings a respondent has (SIBS) increased, so does the number of children they have (CHILDS), shown by the positive linear relationship, indicating a positive correlation. 

```{r, message = FALSE, warning = FALSE}
gss2016 <- data.frame(read.csv("gss-2016.csv"))
ggplot(gss2016, aes(x = SIBS, y = CHILDS)) + geom_point() + geom_smooth(method = "lm") + geom_jitter()
```

B. The covariance between SIBS and CHILDS, accounting for incomplete observations is calculated below:

```{r}
cov(gss2016$SIBS, gss2016$CHILDS, use = "complete.obs")
```
The positive number confirms a positive relationship between SIBS and CHILDS, meaning, as one goes up, so does the other. It does not indicate the strength of the relationship, but it can help in predicting the direction of estimated values. 

C. The correlation test chosen is the Pearson Correlation, as not only is its range specified between -1 and 1, but it can indicate the strength of relationships as well as direction. As the covariance calculation calculates the direction of correlation, positive or negative, I would expect the Pearson test to yield the same results as the calculation performed in section B.

D. The Pearson correlation test is calculated below:
```{r}
cor(gss2016$SIBS, gss2016$CHILDS, use = "complete.obs", method = 'pearson')
```
The positive number further confirms what the covariance indicated - a positive relationship between SIBS and CHILDS. However, it is a small number, closer to 0 than 1, indicating that the correlation is not strong. In order to have a significant relationship, the correlation would need to be .5 or larger - a correlation of .1988582 is weak to moderate at best. 

E. The correlation coefficient is described above at .1988582. The coefficient of determination is simply the square of the correlation coefficient, which is calculated at .03954459. The coefficient of determination gives a measure of variability in one variable that is shared by the other, as outside factors can be affecting the results of how many children a person has. We can use the coefficient of determination to determine the percentage of affect that SIBS has on CHILDS but multiplying by 100%, giving us 3.95%, which means that the number of siblings a respondent can account for only 3.95% of the variability of the number of children a respondent had. This leaves 96.05% to outside factors. 

F. Due to the positive correlation and covariance, I believe the relationship to be positive - as one increases, the other tends to as well; however, due to the small coefficient of determination, I believe that outside factors have a much larger effect on them rather than the number of siblings affecting the number of children a respondent would have. There is a relationship between the two, but it is weak. 

G. Corresponding graph:

```{r, message = FALSE, warning = FALSE}
ggplot(gss2016, aes(x = SIBS, y = CHILDS)) + geom_point() + geom_smooth() + geom_jitter()
```

# Part 3

A. Regression Analysis:

```{r}
SIBMOD <- lm(CHILDS ~ SIBS, data = gss2016)
summary(SIBMOD)
```

B. From the regression analysis, we can see that the intercept is 1.467767, and the slope is .103577. The coefficient of determination is described above at .03954. Since the coefficient of determination is simply the square of the correlation coefficient, we can calculate the the correlation coefficient to be .198847.

C. The coefficient of determination gives a measure of variability in one variable that is shared by the other, as outside factors can be affecting the results of how many children a person has. We can use the coefficient of determination to determine the percentage of affect that SIBS has on CHILDS but multiplying by 100%, giving us 3.95%, which means that the number of siblings a respondent can account for only 3.95% of the variability of the number of children a respondent had. This leaves 96.05% to outside factors.

D. With an F-Ratio of 117.5, this points to a much better model, as we tend to look for an F-Ratio at least higher than 1, and the p-value is less than .001.

E. Using the model, CHILDS = slope(SIBS) + intercept, we can calculate that if a person were to have three siblings, they would be predicted to have 1.778498 children. But as we know that it is not possible to have a percentage of a child, it would round to 2.

F. Using the same model again, we can calculate that if a person has no siblings, they are predicted to have at least 1.467767 children, the intercept. Again, as we cannot have a percentage of a child, it would be rounded to 1.

# Part 4
```{r}
library(dplyr)
library(tidyr)
library(readr)
parse_date("01/02/2010", "%d/%m/%Y")
week7 <- read_csv("week-7-housing.csv",
                  na = c("", "NA", "N/A"),
                  col_types = cols(
                    zip5 = col_factor(levels = NULL),
                    building_grade = col_factor(levels = NULL),
                    present_use = col_factor(levels = NULL),
                    Sale_Date = col_date(format = "%m/%d/%Y"),
                    sale_instrument = col_factor(levels = NULL),
                    sale_warning = col_factor(levels = NULL),
                    sitetype = col_factor(levels = NULL),
                    #year_renovated = col_factor(levels = c(0,1970:1979,1980:1989,1990:1999,2000:2009,2010:2019))
                    ))
```

## Week 7 Housing

A. I have not removed any data points as of yet. 

B. Models for Sale Price vs Square Footage of the Lot, and for Sale Price vs Square Footage of the Lot plus number of Bedrooms and number of Full Bathrooms

```{r}
Price_SqFoot <- lm(Sale_Price ~ sq_ft_lot, data = week7)
Price_BR_FullB <- lm(Sale_Price ~ sq_ft_lot + bedrooms + bath_full_count, data = week7)
```

C. Summaries for Price_SqFoot and Price_BR_FullB:

```{r, echo = FALSE}
summary(Price_SqFoot)
summary(Price_BR_FullB)
```

The R2 and Adjusted R2 for Price_SqFoot are .01435 and .01428, respectively, while the R2 and Adjusted R2 for Price_BR_FullB are .1127 and .1125, respectively. The decreases from R2 to Adjusted R2 are indicators of how well a model fits a curve while adjusting for the number of variables in a model, and the more useless or unrelated an added variable is, the more the Adjusted R2 decreases. As the R2 for Price_SqFoot decreased by .00003 and the R2 for Price_BR_FullB decreased by only .0002, we can tell that as the number of Bedrooms and Full Bathrooms are added to the model, it begins to not fit as well, indicating that the number of Bedrooms and Full Bathrooms are not as related as Square Footage of the Lot is to the Sale Price.

D. The standardized betas for Price_SqFoot and Price_BR_FullB:

```{r}
library(lm.beta)
lm.beta(Price_SqFoot)
lm.beta(Price_BR_FullB)
```

The Standard Deviations for Sale Price, Square Footage of the lot, Number of Bedrooms, and Number of Full Baths:

```{r}
sd(week7$Sale_Price, na.rm = TRUE)
sd(week7$sq_ft_lot, na.rm = TRUE)
sd(week7$bedrooms, na.rm = TRUE)
sd(week7$bath_full_count, na.rm = TRUE)
```

Standardized betas tell us about the relationship between Sale Price and it's predictors. Much like correlation, the positive values indicate positive relationships between Sale Price and Square Footage of the lot, as well as Sale Price and Bedrooms and Full Baths. However, unlike correlation, we can determine the degree that "each predictor affects the outcome if the effects of all other predictors are held constant." Therefore, the standardized beta value of .1198122 for Square Footage of the lot means that for every standard deviation that Square Footage increases, Sale Price increases .1198122 standard deviations. Since one standard deviation of Square Footage of the lot is 56,933.29 feet and one standard deviation of Sale Price is $404,381.10, we know that for ever 56,933.29 increase in Square Footage of the lot, the Sale Price increases 0.1198122($404,381.10) = $48,449.79.

However, when Bedrooms and Full Bathrooms are added to the equation, the standardized beta value of the Square Footage of the lot becomes 0.1017484. While still indicating a positive relationship, the degree of change is now different. As the Square Footage of the lot increases by one standard deviation - 56,933.29 - and the number of bedrooms and full bathrooms stay constant, the Sale Price increases 0.1017484 standard deviation, which is calculated as 0.1017484($404,381.10) = $41,145.13.

Similarly, the standardized beta value of Bedrooms is 0.1492025, indicating a positive relationship where the number of Bedrooms increases by 1 and the Square Footage of the lot and Full Bathrooms stay constant. Using the calculated standard deviation value for Bedrooms, we can see that for every .8761273 increase in Bedrooms while the Square Footage of the lot and number of Full Bathrooms stays constant, the Sale Price increases by 0.1492025($404,381.10)= $60,334.67.

The standardized beta value of Full Bathrooms is 0.2346207, indicating a positive relationship where the number of Full Bathrooms increases by 1 and the Square Footage of the lot and Bedrooms stay constant. Using the calculated standard deviation value for Full Baths, we can see that for every .6507965 increase in Full Baths while the Square Footage of the lot and number of Bedrooms stay constant, the Sale Price increases by 0.2346207($404,381.10) = $94,876.18.

E. The Confidence Intervals for Sale Price, Square Footage of the lot, Number of Bedrooms, and Number of Full Baths:

```{r}
library(Rmisc)
```
```{r}
CI(week7$Sale_Price)
CI(week7$sq_ft_lot)
CI(week7$bedrooms)
CI(week7$bath_full_count)
```

F. The analysis of variance between Price_SqFoot and Price_BR_FullB:
```{r}
anova(Price_SqFoot, Price_BR_FullB)
```

With a Pr(>F) value of 2.2e-16 being a very small number, we can conclude that the second model, Price_BR_FullB significantly improved the fit of the model to the data.

G.

```{r}
week7_nooutliers <- week7 %>%
  filter(sq_ft_lot < 1250000, bath_full_count < 10, bedrooms < 10)
```

H. The standardized residuals of Price_SqFoot and Price_BR_FullB:

```{r}
week7$stresids_Price_SqFoot <- rstandard(Price_SqFoot)
week7$stresids_Price_BR_FullB <- rstandard(Price_BR_FullB)
week7$LGresids_Price_SqFoot <- rstandard(Price_SqFoot) < -2 |rstandard(Price_SqFoot) > 2
week7$LGresids_Price_BR_FullB <- rstandard(Price_BR_FullB) < -2|rstandard(Price_BR_FullB) > 2
```

I. The number of variables with large residuals for Price_SqFoot:
```{r, echo = FALSE}
sum(week7$LGresids_Price_SqFoot)
```
The number of variables with large residuals for Price_BR_FullB:
```{r, echo = FALSE}
sum(week7$LGresids_Price_BR_FullB)
```

J. The variables with large residuals for Price_SqFoot are:
```{r, echo = FALSE}
week7[week7$LGresids_Price_SqFoot, c("Sale_Price", "sq_ft_lot", "stresids_Price_SqFoot")]
```

The variables with large residuals for Price_BR_FullB are:

```{r, echo = FALSE}
week7[week7$LGresids_Price_BR_FullB, c("Sale_Price", "sq_ft_lot", "bedrooms", "bath_full_count", "stresids_Price_BR_FullB")]
```

K. The leverage of Price_SqFoot and Price_BR_FullB, respectively:
```{r}
week7$lev_Price_SqFoot <- hatvalues(Price_SqFoot)
week7$lev_Price_BR_FullB <- hatvalues(Price_BR_FullB)
```

The Cook's Distance of Price_SqFoot and Price_BR_FullB, respectively:
```{r}
week7$cd_Price_SqFoot <- cooks.distance(Price_SqFoot)
week7$cd_Price_BR_FullB <- cooks.distance(Price_BR_FullB)
```

The covariance rations of Price_SqFoot and Price_BR_FullB, respectively:
```{r}
week7$cvratio_Price_SqFoot <- covratio(Price_SqFoot)
week7$cvratio_Price_BR_FullB <- covratio(Price_BR_FullB)
```

L. The Durbin-Watson Test to assert independence of variables is calculated below for Price_SqFoot and Price_BR_FullB, respectively:
```{r, echo = FALSE, include = FALSE}
library(car)
```
```{r, echo = FALSE}
dwt(Price_SqFoot)
dwt(Price_BR_FullB)
```

As both DW values are .7 approximately, we know that the variables in each model are positively correlated and do not meet the condition of independence.

M. As Price_SqFoot only has one predictor, we are unable to run the VIF tests to determine multicollinearity. The tests for Price_BR_FullB are below:

```{r}
vif(Price_BR_FullB)
1/vif(Price_BR_FullB)
mean(vif(Price_BR_FullB))
```

As the VIF values are below 10, the tolerance values are above .2 and the average VIF is fairly close to 1, we can conclude that there is no multicollinearity in our data.

N. Plots and histograms of Price_SqFoot and Price_BR_FullB, respectively
```{r, echo = FALSE, warning = FALSE, message = FALSE}
plot(Price_SqFoot$fitted.values, Price_SqFoot$residuals)
hist(week7$stresids_Price_SqFoot)
plot(Price_BR_FullB$fitted.values, Price_BR_FullB$residuals)
hist(week7$stresids_Price_BR_FullB)
```

In each plot function, you can see an outlier in the bottom right corner, indicating a very large fitted value, but a very small residual. This can affect the plot, causing the remaining values to be concentrated on the far left.

# Part 5


```{r , include=FALSE}
library(foreign)
ThoracicSurgery <- read.arff("ThoraricSurgery (1).arff")
```

## University of California Irvine - Thoracic Surgery Data

A. The summary for the binary logistic regression model is shown below:

```{r, echo = FALSE}
ThorSur_glm <- glm(Risk1Yr ~ ., data = ThoracicSurgery, family = 'binomial')
summary(ThorSur_glm)
```

B. According to the summary, the variables with the greatest effect on the survival rate were DGNDGN8 with a slope of 18.03, DGNDGN4 with a slope of 16.38, and DGNDGN2 with a slope of 14.74.

C. Using the dataset provided, we can create training and test subsets of the data and train a model to predict the accuracy of our model that predicts whether or not a patient survived after one year, given all of the variables:

```{r, include = FALSE}
library(caTools)
```
```{r}
#split the data
split <- sample.split(ThoracicSurgery, SplitRatio = .8)
#create training data subset
train <- subset(ThoracicSurgery, split == "TRUE")
#create test data subset
test <- subset(ThoracicSurgery, split == "FALSE")

#create model of new training data subset
trainmodel <- glm(Risk1Yr ~ ., data = train, family = 'binomial')

#use test data on training model
testpredict <- predict(trainmodel, test, type = "response") 
testpredict <- predict(trainmodel, train, type = "response")

#use confustion matrix to compute accuracy of model
confmatrix <- table(Actual_Value = train$Risk1Yr, Predicted_value = testpredict > .5)
confmatrix
((confmatrix[[1,1]] + confmatrix[[2,2]]) / sum(confmatrix)) * 100
```

According to the confusion matrix, our model is approximately 85% accurate.

# Part 6
```{r, include=FALSE}
library(class)
library(readr)
library(car)
library(mlogit)
binaryclass <- read_csv("binary-classifier-data.csv",
                         col_types = cols(
                           label = col_factor(levels = NULL)
                         ))
binaryclass_glm <- glm(label ~ ., data = binaryclass, family = 'binomial')
```

## Binary Classifier Data

A.
```{r, include = FALSE}
library(caTools)
```
```{r}
train_index <- sample(1:nrow(binaryclass), (nrow(binaryclass)*.8))
test_index <- setdiff(1:nrow(binaryclass), train_index)

#create training data subset
train <- binaryclass[train_index, ]
#create test data subset
test <- binaryclass[test_index, ]

#create model of new training data subset
trainmodel <- glm(label ~ ., data = train, family = 'binomial')

#use test data on training model
testpredict <- predict(trainmodel, test, type = "response") 
testpredict <- predict(trainmodel, train, type = "response")

#use confustion matrix to compute accuracy of model
confmatrix <- table(Actual_Value = train$label, Predicted_value = testpredict > .5)
confmatrix
((confmatrix[[1,1]] + confmatrix[[2,2]]) / sum(confmatrix)) * 100
```

B. The accuracy of the nearest neighbors algorithm is calculated below:

```{r}
train_labels <- binaryclass[train_index, 1, drop = TRUE]
test_labels <- binaryclass[test_index, 1, drop = TRUE]

binclasspred <- knn(train = train, test = test, cl = train_labels, k = 39)
library(gmodels)
CrossTable(x = test_labels, y = binclasspred, prop.chisq = FALSE)
```

We can see that 128 cases were accurately predicted as "0", and 160 cases were accurately predicted as "1", which, for a total amount of cases of 300, is an accurary of 128+160/300 = 96% accuracy. When compared to the regression model, we can see that the nearest neighbor algorithm is much more accurate for this data set, as the regression model was only 50%-55% accuarate.

C. We're seeing an improvement in accuracy from the KNN model versus the logistic regression model for a few reasons. In this data set, we only had two input variables, and the KNN tends to work better on a small number of inputs. Additionally, the KNN does not make assumptions about the functional form of the problem being solved, while logistic regression will always be a Sigmoid curve. Lastly, while it was not specifically analyzed, colinearity and outliers tend to interfere with the accuracy of a logistic regression model more than with a KNN.

# Part 7

```{r, include=FALSE}
library(ggplot2)
library(readr)
binary <- read_csv("binary-classifier-data.csv")
trinary <- read_csv("trinary-classifier-data.csv")
```

## Binary vs. Trinary

A. Scatterplot graph of the binary data:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
ggplot(binary, aes(x = x, y = y)) + geom_point()
```

Scatterplot graph of the trinary data:
```{r, echo = FALSE, message = FALSE, warning = FALSE}
ggplot(trinary, aes(x = x, y = y)) + geom_point()
```

B.Graph of the Accuracy of the Binary and Trinary data with different KNN values:
```{r, include = FALSE}
library(caTools)
```

```{r, include = FALSE}
bi_train_index <- sample(1:nrow(binary), (nrow(binary)*.8))
bi_test_index <- setdiff(1:nrow(binary), bi_train_index)

#create training data subset
bi_train <- binary[bi_train_index, ]
#create test data subset
bi_test <- binary[bi_test_index, ]

bi_train_labels <- binary[bi_train_index, 1, drop = TRUE]
bi_test_labels <- binary[bi_test_index, 1, drop = TRUE]

library(class)
#knn predictions for different K values
bi_pred_3 <- knn(train = bi_train, test = bi_test, cl = bi_train_labels, k = 3)
bi_pred_5 <- knn(train = bi_train, test = bi_test, cl = bi_train_labels, k = 5)
bi_pred_10 <- knn(train = bi_train, test = bi_test, cl = bi_train_labels, k = 10)
bi_pred_15 <- knn(train = bi_train, test = bi_test, cl = bi_train_labels, k = 15)
bi_pred_20 <- knn(train = bi_train, test = bi_test, cl = bi_train_labels, k = 20)
bi_pred_25 <- knn(train = bi_train, test = bi_test, cl = bi_train_labels, k = 25)

#calculate accuracy for each model
bi_pred_3_acc <- 100 * sum(bi_test_labels == bi_pred_3)/nrow(bi_test)
bi_pred_5_acc <- 100 * sum(bi_test_labels == bi_pred_5)/nrow(bi_test)
bi_pred_10_acc <- 100 * sum(bi_test_labels == bi_pred_10)/nrow(bi_test)
bi_pred_15_acc <- 100 * sum(bi_test_labels == bi_pred_15)/nrow(bi_test)
bi_pred_20_acc <- 100 * sum(bi_test_labels == bi_pred_20)/nrow(bi_test)
bi_pred_25_acc <- 100 * sum(bi_test_labels == bi_pred_25)/nrow(bi_test)
```

```{r, include = FALSE}
tri_train_index <- sample(1:nrow(trinary), (nrow(trinary)*.8))
tri_test_index <- setdiff(1:nrow(trinary), tri_train_index)

#create training data subset
tri_train <- trinary[tri_train_index, ]
#create test data subset
tri_test <- trinary[tri_test_index, ]

tri_train_labels <- trinary[tri_train_index, 1, drop = TRUE]
tri_test_labels <- trinary[tri_test_index, 1, drop = TRUE]

#knn predictions for different K values
tri_pred_3 <- knn(train = tri_train, test = tri_test, cl = tri_train_labels, k = 3)
tri_pred_5 <- knn(train = tri_train, test = tri_test, cl = tri_train_labels, k = 5)
tri_pred_10 <- knn(train = tri_train, test = tri_test, cl = tri_train_labels, k = 10)
tri_pred_15 <- knn(train = tri_train, test = tri_test, cl = tri_train_labels, k = 15)
tri_pred_20 <- knn(train = tri_train, test = tri_test, cl = tri_train_labels, k = 20)
tri_pred_25 <- knn(train = tri_train, test = tri_test, cl = tri_train_labels, k = 25)

#calculate accuracy for each model
tri_pred_3_acc <- 100 * sum(tri_test_labels == tri_pred_3)/nrow(tri_test)
tri_pred_5_acc <- 100 * sum(tri_test_labels == tri_pred_5)/nrow(tri_test)
tri_pred_10_acc <- 100 * sum(tri_test_labels == tri_pred_10)/nrow(tri_test)
tri_pred_15_acc <- 100 * sum(tri_test_labels == tri_pred_15)/nrow(tri_test)
tri_pred_20_acc <- 100 * sum(tri_test_labels == tri_pred_20)/nrow(tri_test)
tri_pred_25_acc <- 100 * sum(tri_test_labels == tri_pred_25)/nrow(tri_test)

```

``` {r, echo = FALSE, message = FALSE, warning = FALSE}
#create data frame with results from both data sets, columns: Data Set, K-value, Accuracy
Data_Set <- c("binary", "binary", "binary", "binary", "binary", "binary", "trinary", "trinary", "trinary", "trinary", "trinary", "trinary")
K_Values <- c(3, 5, 10, 15, 20, 25, 3, 5, 10, 15, 20, 25)
Accuracy <- c(bi_pred_3_acc, bi_pred_5_acc, bi_pred_10_acc, bi_pred_15_acc, bi_pred_20_acc, bi_pred_25_acc, tri_pred_3_acc, tri_pred_5_acc, tri_pred_10_acc, tri_pred_15_acc, tri_pred_20_acc, tri_pred_25_acc)

Pred_df <- data.frame(Data_Set, K_Values, Accuracy)

#plot K-values on x, Accuracy on y, Data Set as color/line
ggplot(Pred_df, aes(x = K_Values, y = Accuracy, color = Data_Set)) + geom_point() + geom_smooth(method = "lm", se = FALSE)
```

C. I don't believe that a linear classifier would work well on these data sets, as they each have plots that could overlap each other, rather than be segregated by a line to classify each set.

# Part 8

## Clustering Data

A. Scatter plot of the cluster data:

```{r, include = FALSE}
library(readr)
library(ggplot2)
library(factoextra)
library(purrr)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
cluster <- data.frame(read_csv("clustering-data.csv"))
ggplot(cluster, aes(x = x, y = y)) + geom_point()
```

B. Scatter Plots of data fitted with k-means algorithm using K values 2 through 12:

```{r, message = FALSE, warning = FALSE}
K2 <- kmeans(x = cluster, 2)
K3 <- kmeans(x = cluster, 3)
K4 <- kmeans(x = cluster, 4)
K5 <- kmeans(x = cluster, 5)
K6 <- kmeans(x = cluster, 6)
K7 <- kmeans(x = cluster, 7)
K8 <- kmeans(x = cluster, 8)
K9 <- kmeans(x = cluster, 9)
K10 <- kmeans(x = cluster, 10)
K11 <- kmeans(x = cluster, 11)
K12 <- kmeans(x = cluster, 12)
```

```{r}
fviz_cluster(K2, geom = "point", data = cluster) + ggtitle("K = 2")
fviz_cluster(K3, geom = "point", data = cluster) + ggtitle("K = 3")
fviz_cluster(K4, geom = "point", data = cluster) + ggtitle("K = 4")
fviz_cluster(K5, geom = "point", data = cluster) + ggtitle("K = 5")
fviz_cluster(K6, geom = "point", data = cluster) + ggtitle("K = 6")
fviz_cluster(K7, geom = "point", data = cluster) + ggtitle("K = 7")
fviz_cluster(K8, geom = "point", data = cluster) + ggtitle("K = 8")
fviz_cluster(K9, geom = "point", data = cluster) + ggtitle("K = 9")
fviz_cluster(K10, geom = "point", data = cluster) + ggtitle("K = 10")
fviz_cluster(K11, geom = "point", data = cluster) + ggtitle("K = 11")
fviz_cluster(K12, geom = "point", data = cluster) + ggtitle("K = 12")

```

C. Line chart of average distances from center for each K Value:

```{r}
wss <- function(k) {kmeans(cluster, k, nstart = 12)$tot.withinss}
k.values <- 2:12
wss_values <- map_dbl(k.values, wss)
plot(k.values, wss_values, type = "b", pch = 19, frame = FALSE, xlab = "K Value", ylab = "Average distance")
```

D. Using the above graph, the elbow point appears to be at the K Value point of 5. This means that the optimal number of clusters to calculate when using the K-means algorithm is 5.

